# ğŸ™ï¸ Chatbot: Upload Link to Get Answer

A Retrieval-Augmented Generation (RAG) chatbot that extracts content from any webpage, stores it in a FAISS vector database, and answers user questions using Hugging Face-hosted LLMs. Designed for compliance-focused, context-grounded responses.

---

**#ğŸš€ Features**

- ğŸ”— Accepts any public webpage URL
- ğŸ§  Extracts and cleans page content using BeautifulSoup
- ğŸ“š Splits text into semantic chunks for vector storage
- ğŸ” Retrieves relevant context using FAISS
- ğŸ¤– Answers questions using Hugging Face's Zephyr-7B LLM
- ğŸ” Secure token handling via `.env`
- ğŸ§¼ Cleans noisy dialogue markers and boilerplate text

---

**#ğŸ§° Tech Stack**

| Component        | Tool/Library                          |
|------------------|---------------------------------------|
| Embeddings       | `sentence-transformers/all-MiniLM-L6-v2` via LangChain |
| Vector Store     | FAISS (CPU)                           |
| LLM              | `HuggingFaceH4/zephyr-7b-beta` via Hugging Face Hub |
| Web Scraping     | BeautifulSoup + Requests              |
| Prompting        | LangChain `ChatPromptTemplate`        |
| Environment Vars | `python-dotenv`                       |

---

**#ğŸ“ Project Structure**

chatbot-audio-to-answer/
â”‚
â”œâ”€â”€ llm_key.py          # Loads HF token, sets up embeddings + chat model
â”œâ”€â”€ web_extract.py      # Scrapes and cleans webpage text
â”œâ”€â”€ model.py                        # RAG pipeline: FAISS + prompt + chat model
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ .env                # Stores HF_TOKEN (not committed)
â”œâ”€â”€ .gitignore          # Ignores .env, venv, pycache
â””â”€â”€ README.md                      # You're reading it!

**#ğŸ”‘ Hugging Face Setup**

1. Create an account at [huggingface.co](https://huggingface.co)
2. Go to [Settings â†’ Access Tokens](https://huggingface.co/settings/tokens)
3. Create a token with **Inference permissions**
4. Create a `.env` file in your project root:
   ```env
   HF_TOKEN=hf_your_token_here

   
**#âš™ï¸ Installation**

# Clone the repo
git clone https://github.com/soumya14/chatbot-audio-to-answer.git
cd chatbot-audio-to-answer

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt


**#ğŸ§ª How to Use**

# Run the pipeline
python model.py

Youâ€™ll be prompted to:

Enter a URL (e.g. https://en.wikipedia.org/wiki/Sachin_Tendulkar)

Enter your question (e.g. "What is the birth place of Sachin Tendulkar?")

Get a grounded answer based on the page content

**#ğŸ§¼ Cleaning Logic**

The system automatically removes:

Dialogue markers like Human:, User:, [/ASS]

Boilerplate like Subscribe, Advertisement, etc.

Truncated or irrelevant content

**#ğŸ“Œ Notes**
This project uses Hugging Faceâ€™s hosted LLMs via InferenceClient

Responses are grounded in retrieved context only

If the answer is not found, the model replies: "Not enough information."

**#ğŸ“„ License**
This project is licensed under the MIT License. Feel free to fork, modify, and build upon it.

ğŸ™Œ Credits
Built by Soumyadip  
Powered by LangChain, Hugging Face, and FAISS

